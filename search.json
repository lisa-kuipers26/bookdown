[{"path":"index.html","id":"introductie","chapter":"1 Introductie","heading":"1 Introductie","text":"Hallo! Ik ben Lisa Kuipers, 22 jaar en studeer momenteel life science op Hogeschool Utrecht. Naast mijn major, die vooral verschillende lab technieken gaat, heb ik ook een minor data science gedaan. dit portfolio wordt er vooral gericht op mijn data science vaardigheden.Tijdens de opleiding hebben veel gebruik gemaakt van github, vooral met oog op version control, workflow en samenwerken. Dit portfolio wordt onder ander gehost met github pages. De opzet van het portfolio kan terug gevonden worden deze github pagina.","code":""},{"path":"verwerken-van-data.html","id":"verwerken-van-data","chapter":"2 Verwerken van data","heading":"2 Verwerken van data","text":"Het interpreteren en verwerken van data van iemand anders belangrijk de data science. Er wordt hier data gebruikt van een onderzoek waar C. elegans nematoden blootgesteld aan verschillende soorten stoffen.Belangrijkste een data analyse bergrijpen waar je data gaat. Door de metadata te lezen krijg je een goed idee van waar je dataset overgaat en wat elke kopje betekent. Voor dit onderzoek willen het effect weten van verschillende stoffen op de nematode maar ook de het effect van verschillende concentraties. Om dit te onderzoeken worden de kolommen RawData, compConcentration en expType gebruikt. RawData staan het aantal nematodes op de plaat, de compConcentration de concentratie van de stof en de expType de soort stof.Om globaal een idee te krijgen van het effect worden deze variabel tegen elkaar uitgezet de grafiek hieronder. Omdat log niet werkt bij een getal van 0 negatieve controle manueel op 0 gezet.het experiment de positieve controle van dit experiment ethanol . De negatieve controle van dit experiment de S-mediumBij het verwerken van data het belangrijk dat de data goed ingelezen wordt. Eén ding waar tegen lopen bij deze dataset dat de compConcentration ingelezen wordt als character type .p.v. een getal. Hierdoor klopt de x-niet met wat je wilt zien, omdat er groepen en geen schaal weergegeven wordt. Dit wordt opgelost door het typ evan compConcentration te veranderen van een character naar een double.De bovenstaande grafiek geeft een globaal overzicht, maar er kunnen geen conclusies uitgetrokken worden. Hiervoor een verdere data analyse nodig. Zo’n data analyse kan er als volgt uitzien:Data plotten zodat er een globaal overzicht hebt.Nul hypothese voor shapiro wilks test (Data normaal verdeeld)Bij >0,05 nulhypothese wordt aangenomen, de data normaal verdeeld. <0,05 nul-hypothese wordt verworpenBij een normaal verdeelde dataset kan er een pearson test uitgevoerd worden.Nulhypothese van pearson test dat er geen verband tussen de concentratie en aantal nematode op de plaat.Nulhypothese <0,05 nulhypothese wordt verworpen, er een correlatie tussen de concentratie en aantal nematode op de plaat. >0,05 Er geen verbandNatuurlijk wordt er bij zes alleen een berekening gedaan en er kunnen natuurlijk ook andere factoren geweest zijn die de uitslag beïnvloed hebben.Bij de grafiek hierboven de dataset genormaliseerd door de RawData te delen daar het gemiddelde van de negatieve controle. Dit gedaan omdat de concentratie en aantal nematode een grote range zit. Op deze manier kunnen hogere getallen de uitslag van analyses onterecht beïnvloeden. Door het op deze manier te normaliseren wordt de range kleiner en er minder bias.","code":"\nlibrary(DT)\nlibrary(kableExtra)\nlibrary(readxl)\nlibrary(tidyverse)\ntabel <- read_excel(here::here(\"data/CE.LIQ.FLOW.062_Tidydata.xlsx\"))\ntabel$compConcentration <- as.double(tabel$compConcentration)\ntabel$compName <- as.factor(tabel$compName)\n#x-as concentreatie in -10log\ntabel$compConcentration <- ifelse(tabel$compConcentration==0,0,-log10(tabel$compConcentration))\ntabel %>% ggplot(aes(x=compConcentration,y=RawData,color=compName,shape=expType))+\n  geom_jitter(width=0.1)+\n  labs(x=\"Concentratie (-log10) \",\n       y=\"Number of offspring\")\n#Genormalizeerd en gemiddelde naar 1\nneg_controle <- tabel %>% filter(expType==\"controlNegative\")\ntabel$RawData <- tabel$RawData / mean(neg_controle$RawData)\ntabel %>% ggplot(aes(x=compConcentration,y=RawData,color=compName,shape=expType))+\n  geom_jitter(width=0.1)+\n  labs(x=\"Concentratie (-log10) \",\n       y=\"Number of offspring\")"},{"path":"open-peer-review.html","id":"open-peer-review","chapter":"3 Open Peer Review","heading":"3 Open Peer Review","text":"","code":""},{"path":"open-peer-review.html","id":"onderzoek-op-reproduceerbaarheid-beoordelen","chapter":"3 Open Peer Review","heading":"3.1 Onderzoek op reproduceerbaarheid beoordelen","text":"dit gedeelte wordt een artikel beoordeeld op de reproduceerbaarheid. Deze wordt beoordeeld aan de hand van verschillende criteria omschreven de onderstaande tabel. Voor de beoordeling een artikel gekozen primair onderzoek dat beschikbaar op PMC.Gebruikte artikel:Amawi KF, Alkhatib AJ. Urtica Pilulifera Treating Pre-diabetic Rat Model Control Blood Glucose, Lipids Oxidative Stress. Med Arch. 2020;74(3):168-171. doi:10.5455/medarh.2020.74.168-171https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7405998/","code":""},{"path":"open-peer-review.html","id":"omschrijving-onderzoek","chapter":"3 Open Peer Review","heading":"3.1.1 Omschrijving onderzoek","text":"Het doel van het onderzoek kijken Urtica pilulifera (een plant) effect heeft op pre-diabetische ratten en ook de antioxiderende werking onderzoekenDe ratten werden ingedeeld drie groepen van 10; een controle groep, een pre diabetische groep, en een groep met de behandeling van U. pilulifera.\nDe ratten pre-diabetisch gemaakt kregen een hoog sucrose dieet, de controle groep een normaal dieet en de behandelde groep kreeg hetzelfde dieet als de pre-diabetische ratten met U. pilulifera extract geïnjecteerd. Na 30 dagen werden bloed samples afgenomen en getest op glucose, triglycerides, cholesterol, GSH, TAC en MDAUit het onderoek bleek dat glucose, triglyceride en MDA niveaus de pre-diabetic groep significant verhoogd waren en significant verlaagd de U. pilulifera groep.\nGSH en TAC significant hoger de U. pilulifera ten opzichte van de pre-diabetische groep.\nEr zat geen significant verschil cholesterol niveau de groepen.","code":""},{"path":"open-peer-review.html","id":"beoordeling-artikel","chapter":"3 Open Peer Review","heading":"3.1.2 Beoordeling artikel","text":"de tabel staat het artikel aan de verschillende criterium voldoet.Het blijkt dat het artikel maar aan drie van acht criteria voldoet. Hoewel het artikel goed te lezen deze dus toch niet goed reproduceerbaar.","code":""},{"path":"open-peer-review.html","id":"code-zelf-reproduceren","chapter":"3 Open Peer Review","heading":"3.2 Code zelf reproduceren","text":"Net er gekeken reporduceerbaarheid van een primair onderzoek. dit onderdeel wordt er gekeken naar de reporduceerbaarheid van (R) code.Data en code van onderstaand onderzoek gebruikt:\nStrozza C, Myrskylä M. Monitoring trends differences COVID-19 case-fatality rates using decomposition methods: Contributions age structure age-specific fatality. PLoS One. 2020 Sep 10;15(9):e0238904. doi: 10.1371/journal.pone.0238904. PMID: 32913365; PMCID: PMC7482960.\nhttps://pubmed.ncbi.nlm.nih.gov/32913365/Link naar de code:\nhttps://osf.io/g7vjd/","code":""},{"path":"open-peer-review.html","id":"beoordeling-script","chapter":"3 Open Peer Review","heading":"3.2.1 Beoordeling script","text":"Het script zelf goed te lezen. Er waren 4 scripts die elkaar opvolgde, door de getallen de bestandsnaam het makkelijk te zien welke als eerst uitgevoerd moest worden. de scripts zelf stonden genoeg comments om te begrijpen waar elk stuk code voor diende en ziet de code zelf er netjes uit. Ik geef de leesbaarheid dus ook een 5/5De code ook goed reproduceerbaar. Er alleen één probleem wat handmatig opgelost moest worden, dit probleem en hoe deze opgelost staat beschreven hieronder. Hierdoor krijgt de reproduceerbaarheid een 4/5, want nadat dit probleem opgelost , konden alle tabellen makkelijk gemaakt worden.","code":""},{"path":"open-peer-review.html","id":"probleem-in-script","chapter":"3 Open Peer Review","heading":"3.2.2 Probleem in script","text":"Bij het uitvoeren van de code hierboven kwam de onderstaande foutmelding:de foutmelding een html bestand te zien. Dit hoort natuurlijk niet bij een read_csv() functie. de code wordt het bestand van een online directory gedownload. De URL staat de code en wanneer deze bezocht werd stond er dit bericht:Het bestand bevindt zich niet meer op de plek waar de URL naar verwees en kon dus ook niet gedownload worden waardoor de html van de pagina werd overgenomen. Er echter wel een menu aan de zijkant aanwezig waar het mogelijk om bij de github van het project te komen. Hier vond ik het output bestand wat nodig voor de code en heb deze handmatig gedownload en toegevoegd aan de /data directory. Nadat deze de directory stond kon het script en de rest van de scripts vlekkeloos uitgevoerd worden.\nHieronder staat het script met de aanpassing. De rest van het script kan gevonden worden op de bovenstaande link. De output van het script zijn 6 verschillende excel bestanden met data het verloop van COVID-19 6 verschillende landen.","code":"\n### Monitoring trends and differences in COVID-19 case fatality  ##############\n### rates using decomposition methods: A demographic perspective ##############\n\n  ### Last updated: 2020-07-16 09:27:20 CEST\n  \n  ### Contact:\n  ### riffe@demogr.mpg.de\n  ### acosta@demogr.mpg.de\n  ### dudel@demogr.mpg.de\n\n\n### Get data ##################################################################\n\n  # Required packages\n  source((\"R/00_functions.R\"))\n\n  # URL + filename\n  \n  url <- 'https://osf.io/wu5ve//?action=download'\n  filename <- 'Data/Output_10.csv'\n  \n  # Load data\n  GET(url, write_disk(filename, overwrite = TRUE))\n  dat <- read_csv(filename,skip=3)\n### Monitoring trends and differences in COVID-19 case fatality  ##############\n### rates using decomposition methods: A demographic perspective ##############\n\n  ### Last updated: 2020-07-16 09:27:20 CEST\n  \n  ### Contact:\n  ### riffe@demogr.mpg.de\n  ### acosta@demogr.mpg.de\n  ### dudel@demogr.mpg.de\n\n\n### Get data ##################################################################\n\n  # Required packages\n  source((\"R/00_functions.R\"))\n\n  # URL + filename\n  filename <- 'data/Output_10.csv'\n  \n  # Load data\n  dat <- read_csv(filename,skip=3)\n  \n### Edit data (select countries, etc.) ########################################\n  \n  # Lists of countries and regions\n  countrylist <- c(\"China\",\"Germany\",\"Italy\",\"South Korea\",\"Spain\",\"USA\")\n  region <- c(\"All\",\"NYC\")\n  \n  # Restrict\n  dat <- dat %>% filter(Country %in% countrylist & Region %in% region)\n  \n  # Remove Tests variable\n  dat <- dat %>% mutate(Tests=NULL)\n  \n  # Drop if no cases/Deaths\n  dat <- na.omit(dat)\n  \n  \n### Save ######################################################################\n  \n  write_csv(dat,file=\"Data/inputdata.csv\")"},{"path":"data-organiseren.html","id":"data-organiseren","chapter":"4 Data organiseren","heading":"4 Data organiseren","text":"Het organizeren van data belangrijk voor reproduceerbaarheid en algemene goede workflow. Hieronder staat een voorbeeld van een map met verschillende projecten. Elke map heeft dezelfde indeling, bij de data en scripts zijn README bestanden toegevoegd waarin metadata staat het project en de bestanden. Ik vind het zelf fijn dat wanneer er veel van één soort bestand (zoals bij bam en fastq bestanden) dat deze een aparte map staan. De rest bestanden staan gewoon de /data.Hieronder staat een voorbeeld van de readme die ik de repo van dit portfolioo heb staan. Ik beschrijf kort wat er de datasets staat en waarvoor ze gebruikt. Ook staat er waar ik ze vandaan gehaald heb.\n(#fig:readme voorbeeld)README voorbeeld bij data bestanden\n","code":"## data/daur2\n## ├── metagenomics_formatief\n## │   ├── README.md\n## │   ├── data\n## │   │   ├── HU_waternet_MOCK2_composition.csv\n## │   │   └── README.md\n## │   ├── metagenomics_formatief.Rmd\n## │   ├── metagenomics_formatief.html\n## │   └── metagenomics_reader.Rproj\n## ├── metagenomics_reader\n## │   ├── README.md\n## │   ├── data\n## │   │   ├── HU_waternet_MOCK1_composition.csv\n## │   │   └── README.md\n## │   ├── metagenomics_reader.Rmd\n## │   ├── metagenomics_reader.Rproj\n## │   └── metagenomics_reader.html\n## ├── rna_seq_airway\n## │   ├── README.md\n## │   ├── data\n## │   │   ├── README.md\n## │   │   └── ipsc_sampledata.csv\n## │   ├── rnaseq_airway.Rmd\n## │   ├── rnaseq_airway.Rproj\n## │   └── rnaseq_airway.html\n## ├── rna_seq_ipsc\n## │   ├── README.md\n## │   ├── data\n## │   │   ├── README.md\n## │   │   └── ipsc_sampledata.csv\n## │   ├── rnaseq_ipsc.Rmd\n## │   ├── rnaseq_ipsc.Rproj\n## │   └── rnaseq_ipsc.html\n## └── rna_seq_onecut\n##     ├── README.md\n##     ├── data\n##     │   ├── README.md\n##     │   └── onecut_sampledata_OC2.csv\n##     ├── rnaseq_onecut.Rmd\n##     ├── rnaseq_onecut.Rproj\n##     └── rnaseq_onecut.html"},{"path":"flow-cytometry-analyse.html","id":"flow-cytometry-analyse","chapter":"5 Flow cytometry analyse","heading":"5 Flow cytometry analyse","text":"32 uur hebben de kans gekregen om een eigen vaardigheid te leren en te ontwikkelen. Daarbij moesten vooral nadenken wat zelf willen en wat van toepassing op onze eigen toekomst. het begin dacht ik dat ik wel meer naar de bioinformatica kant op wou, maar naar een paar weken besefte ik dat ik het leuker vind om op het lab kan staan en mijn informatica vaardigeheden kan inzetten bij verschillende analyses en visualisatie. Hierdoor stapte ik van mijn orginele plan af wat meer technisch en ging kijken naar welke lab analyses uitgebreid R gedaan kunnen worden.Mij intresse ligt vooral de immunologie. Ik ging naar de Bioconductior website om een beetje inspiratie op te doen. Na verschillende workflows op bioconductor bekeken te hebben heb ik uiteindelijk voor de flowcytometrie gekozen. Dit een een techniek wat ik ken van praktijk lessen en die veel gebruikt wordt de immunologie.Om deze analyse workflow te leren er een stappenplan opgesteld","code":""},{"path":"flow-cytometry-analyse.html","id":"introductie-wat-is-flow-cytometrie","chapter":"5 Flow cytometry analyse","heading":"5.1 Introductie: Wat is flow cytometrie","text":"Met flowcytometrie komen stoffen opgelost een buffer langs verschillende lazers. Er wordt gekeken naar de lichtbreking en naar de fluoriscentie. De grootte van een cel kan bepaald worden door een forward scatter. Met een hoek van 90 graden kan de granuliteit bekeken worden.fluoriscentie labels die (vaak) aan antilichamen vast zitten kan ervoor zorgen dat er meer complexerere informatie onderzocht kan worden. Deze kunnen bijvoorbeeld binden aan receptoren op het cell membraan en op deze manier kan er bepaald worden wat voor cel precies aanwezig . Dit maakt het interessant voor immunologie onderzoek, omdat cellen zoals T-cellen, B-cellen, NK-cellen, etc onderzocht kunnen worden de hoeveelheid er aanwezig bij een bepaald immuunrespons.Naast flow cytometrie wardt de laatste jaren ook mass cytometry gebruikt. Mass cytometry maakt geen gebruik van forward en side scatter en fluoriscentie, maar van antilichamen met metaal ionen en gebasseerd op mass spectrometry time--flight. (???)","code":""},{"path":"flow-cytometry-analyse.html","id":"de-workflow","chapter":"5 Flow cytometry analyse","heading":"5.2 De workflow","text":"Voor het analyseren van flow cytometry de workflow van (???) gebruikt. De workflow bestaat uit verschillende packages. Door BioManager te installeren kunnen deze packages geïnstalleerd worden met BiocManager::install(\"cytofWorkflow\"). de workflow staat beschreven hoe de packages gebruikt worden maar het zeker ook handig om de individuele documentatie te lezen. Belangrijke packages zijn: CATALYST voor het maken van een SingleCellExperminent class, flowSOM voor het maken van verschillende plots en ConsensusClusterPlus voor de visualisatie van meta clustering.","code":""},{"path":"flow-cytometry-analyse.html","id":"datasets","chapter":"5 Flow cytometry analyse","heading":"5.3 Datasets","text":"Tijdens de tutorial gebruikt gemaakt van de package HDCytoData, om cytometrie data op te halen. De dataset wordt omgezet een SummarizedExperiment en flowSet format zodat deze gebruikt kunnen worden met bioconductor en R. Voor mijn analyse wou ik een externe dataset gebruiken met een onderzoek wat mij leuk lijkt. Er zijn verschillende websites waar cyTOF datasets gevonden kunden worden zoals Cytobank en Immport. Ikzelf heb voor Immport gekozen, omdat het de makkelijkste om datasets te zoeken met veel informatie aanwezig. Het uikiezen van een onderzoek bleek echter niet zo makkelijk te zijn. De datasets zijn al snel heel groot en dit kost veel tijd om te downloaden en R heeft niet genoeg geheugen om het te verwerken. Zelfs kleinere datasets kosten veel geheugen en mijn laptop kon dat simpelweg niet aan. Vooral bij het maken van het sce object ging het steeds fout.Uiteindelijk heb ik gekozen voor het onderzoek van (???). Hoewel het uiteindelijk lukte door mijn RAM te vergroten met memory.limit(size = 40000) duurde het heel lang voordat een graiek gegenereerd kon worden. De workflow maakte gebruik van de HDCytoData package om bestanden op te halen, maar de bestanden deze package hebben de metadata en panel data op https://flowrepository.org/ staan en deze website op het moment van het schrijven van dit script niet bereikbaar.De code van het inlezen van externe data staat hieronder.De workflow tutorial gebruikt data bestanden met een formaat die al geacepteerd worden met de functies uit de CATALYST package die de volgende stuk code aan bod komen. Om de code te laten werken worden de tabellen die aanwezig zijn omgezet dataframes met andere namen.","code":"\nlibrary(flowCore)\n\nflowset <- read.flowSet(path=\"data/CyTOF_result\",truncate_max_range = FALSE)\n\nmetadata_raw <- read.delim(\"data/SDY1647-DR44_Subject_2_CyTOF_result.txt\",sep = \"\\t\")\nmetadata <- data.frame(\"file_name\"=metadata_raw$File.Name,\n                     \"sample_id\"=metadata_raw$Expsample.Accession,\n                     \"patient_id\"=metadata_raw$Subject.Accession,\n                     \"condition\"=metadata_raw$ARM.Name)\n\npanel_raw <- read.delim(\"data/fcs_header_marker.txt\",sep = \"\\t\")\npanel <- data.frame(\"fcs_colname\"=panel_raw$PNN_REPORTED,\n                    \"antigen\"=panel_raw$PNS_REPORTED)\n\nall(panel$fcs_colname %in% colnames(flowset))\nlibrary(CATALYST)\n\nmetadata$condition <- factor(metadata$condition, levels = c(\"healthy\", \"HESN\"))\nmetadata$sample_id <- factor(metadata$sample_id, levels = metadata$sample_id[order(metadata$condition)])\n\nsce <- prepData(flowset, panel, metadata, features = panel$fcs_colname)"},{"path":"cv.html","id":"cv","chapter":"6 cv","heading":"6 cv","text":"0612345678lisa.kuipers26@hotmail.comRembrandt college Veenendaal | 2012 - 2017HAVO diploma gehaald met het profiel natuur en gezondheidHogeschool Utrecht | 2019 – Heden\nHBO Biologie en medisch laboratoriumonderzoek (life science)Theorie geleerd cellen, moleculaire biologie, micro-organismes, farmacologie, laboratorium technieken, hematologie, fysiologie en immunologie. Op het lab heb ik verschillende laboratorium vaardigheden geleerd op het gebied van eiwit analyse(westernblot, SDS-PAGE, ELISA), bloed onderzoek, DNA/RNA(PCR, elektroforese) en bacterie onderzoek (gram kleuring, antibiotica resistentie)Specialisatie biomolecular research\nMinor data science biology\n_ Geleerd om te werken met grote biologische datasets, verschillende analyses uit te voeren met de programmeer R (RNAseq, metagenomics) en hoe er reproduceerbaar gewerkt kan worden.\nMinor data science biology\n_ Geleerd om te werken met grote biologische datasets, verschillende analyses uit te voeren met de programmeer R (RNAseq, metagenomics) en hoe er reproduceerbaar gewerkt kan worden.HEMA Rhenen | 2015 – heden\nKassa medewerker, horeca medewerkerKlanten helpen de winkel, producten afrekenen en helpen op de klantenservice\nKlanten bedienen de horeca\nAanvullen van producten de winkel\nKlanten bedienen de horecaAanvullen van producten de winkelNederlands\nEngelsAlgemeen:\nMicrosoft officeProgrammeer vaardigehden:\nR\nBash\nPython\nSQLLab vaardigheden:\nWesternbot\nSDS-PAGE\nELISA\nPCR\nelektroforese","code":""},{"path":"het-citeren-van-andere-werken.html","id":"het-citeren-van-andere-werken","chapter":"7 Het citeren van andere werken","heading":"7 Het citeren van andere werken","text":"Het citeren van werken van belangrijk om fraude tegen te gaan, maar ook om je uitspraken meer geloofwaardig te laten overkomen. Hieronder staat een stuk tekst waar naar verschillende bronnen verwezen wordt.Het maxima center Utrecht houdt zich bezig met het onderzoeken en behandelen van kinderoncologie. Zo worden er verschillende technieken ontwikkeld en getest om een zo goed mogelijke behandeling te geven aan een patiënt. Zo er een onderzoek naar leukemie kinderen een antilichaam gebruikt die DNA bindingen kan verbreken en de kankercellen dus dood gaan.(Brivio et al. 2021). Een andere techniek die gebruikt wordt high-throughput screening (HTS). HTS een techniek waarbij er een groot aantal stoffen getest één keer getest kunnen worden. Dit levert grote hoeveelheden data op, en de kans van het vinden van een effectief medicijn wordt vergroot. Samen met verschillende fluorescentie technieken en kunnen er tot wel 100000 samples gescreend worden per dag (Liu, Li, Hu 2004). Hoewel er het begin van de ontwikkeling van de HTS techniek vooral gezorgd werd om het aantal assays omhoog te krijgen, wordt er tegenwoordig vooral gekeken naar assays met meer fysiologische relevantie zodat er minder tests gedaan hoeven te worden en kosten lager kunnen worden(Mayr Bojanic 2009). Het prinses maxima center hebruikte HTS onder andere om een effectieve drug te vinden tegen acute myeloïde leukemie. De drug Pyrvinium pamoate bereikte een IC50 bij concentratie onder de 80 nM, en reduceerde cel groei en deling met 50% (Wander et al. 2021).Om HTS mogelijk te maken wordt bij het prinses maxima wordt gebruik gemaakt van het Beckman Coulter Biomek i7 Hybrid werkstation, waar het voorbereiden van de platen plaats vindt en er grote snel- en nauwkeurigheid een 384 wells plaat gevuld kan worden (“High-throughput Screening” n.d.). de wells zitten verschillende soorten medicatie controles, Het bijhouden van wat er welke well zit werd tot op het heden met de hand Excel gemaakt. Het maxima center vond dat dit makkelijker kon en heeft ons de opdracht gegeven om een app te maken waar deze informatie sneller en nauwkeuriger ingevoerd kan worden. Door gebruik te kunnen maken van de programmeertaal R wordt de app gemaakt met de shiny package. de app zelf het mogelijk om layouts te loaden en deze aan te passen naar de huidige opzet. Zo kan de onder andere de drug, concentratie en de aanwezigheid van cellen aangepast worden. Met visualisatie plots het mogelijk om deze aanpassing te zien en door de keuzes standaard te maken, worden fouten sneller voorkomen. Nadat alle aanpassing gemaakt zijn kan het bestand gedownload worden en staat de nieuwe data een .CSV bestand.Brivio, Erica, Franco Locatelli, Marta Lopez-Yurda, Andrea Malone, Cristina Díaz-de-Heredia, Bella Bielorai, Claudia Rossig, et al. 2021. “Phase 1 Study Inotuzumab Ozogamicin Pediatric Relapsed/Refractory Acute Lymphoblastic Leukemia (ITCC-059 Study).” Blood 137 (12): 1582–90. https://doi.org/10.1182/blood.2020007848.“High-throughput Screening.” n.d. Research. Accessed June 11, 2022. https://research.prinsesmaximacentrum.nl/nl/kernfaciliteiten/high-throughput-screening.Liu, Bailing, Songjun Li, Jie Hu. 2004. “Technological Advances High-Throughput Screening.” American Journal Pharmacogenomics 4 (4): 263–76. https://doi.org/10.2165/00129785-200404040-00006.Mayr, Lorenz M, Dejan Bojanic. 2009. “Novel Trends High-Throughput Screening.” Current Opinion Pharmacology, Anti-infectives/New technologies, 9 (5): 580–88. https://doi.org/10.1016/j.coph.2009.08.004.Wander, Priscilla, Susan T. C. J. M. Arentsen-Peters, Sandra S. Pinhanҫos, Bianca Koopmans, M. Emmy M. Dolman, Rijndert Ariese, Frank L. Bos, et al. 2021. “High-Throughput Drug Screening Reveals Pyrvinium Pamoate Effective Candidate Pediatric MLL-Rearranged Acute Myeloid Leukemia.” Translational Oncology 14 (5): 101048. https://doi.org/10.1016/j.tranon.2021.101048.","code":""},{"path":"databases.html","id":"databases","chapter":"8 Databases","heading":"8 Databases","text":"Databases worden gebruikt om veel data op te slaan. Deze data makkelijk op te zoeken met de codeer taal SQL. Op deze pagina wordt weergegeven hoe databases met R gekoppeld kunnen worden en worden er een paar stukken SQL code getoond.","code":""},{"path":"databases.html","id":"data-voorbereiden","chapter":"8 Databases","heading":"8.1 Data voorbereiden","text":"Om wat verschillende functies van de database te laten zien worden er verschillende datasets ingeladen. Hieronder wordt de data van de dengue en flu google trends tidy gemaakt, door de landnamen één kolom te zetten. Ook wordt er een aparte kolom gemaakt met de naam van de ziekte. Voorderest worden ook de datum opgesplitst dagen, maanden en jaren zodat deze later samengevoegd kunnen worden met de gapminder data.Er moet een database gemaakt worden om de tabellen op te slaan. Hieronder staat SQL code die ingevoerd kan worden bij het programma dBeaver (een andere database software) voor het creëren van een database.Om de database te gebruiken R wordt er een connectie aangemaakt. Met de connectie kan R code gebruikt worden voor interactie met de database. Dit handig omdat er dan niet steeds tussen twee programma’s hoeft te switchen. Via de connectie worden de verschillende tabellen ingeladen de database.Om de gapminder wat netter te maken heb ik ervoor gekozen om alvast de landen en jaren die niet de flu en dengue google trend data voorkomt te verwijderen.De oude tabel gapminder wordt via de connectie verwijderd en de nette wordt ingevoerdSQL code erg letterlijk en makkelijk leesbaar. Om de flu en dengue datasets meer tidy te maken wordt de functie UNION gebruikt. Deze functie zorgt ervoor dat twee tabellen die dezelfde kolommen hebben samen worden gevoegd. Omdat alle kolommen overeenkomen kan er een sterretje gebruikt worden bij select om alle kollommen mee te nemen. Het eindresultaat een tabel genaamd flu_dengue waarin de informatie van de twee ziektes gecombineerd staat.De tabel die net aangemaakt wordt samengevoegd met de gapminder dataset door een join te gebruiken. dit geval een left join op jaar, omdat gapminder alleen per jaar gaan en niet maanden en dagen. Door bij select worden de kollommen gekozen, waarbij alles van gapminder die ook overlap heeft met flu_dengue en de kolommen die alleen flu_dengue aanwezig zijn. Daarbij gejoined met land zodat de dataset netter geordend . Aan het einde wordt er geordend op jaar, land, maand en dag. Het resultaat een tabel genaamd all_data en deze gaat gebruikt worden voor de komende analyses.De tabel wordt opgehaald uit de database om deze R te gebruiken en opgeslagen als een RDS voor eventueel later gebruik. De connectie met de database niet meer nodig dus wordt deze afgesloten.","code":"\nlibrary(dslabs)\nlibrary(tidyverse)\n\n#laden van gapminder\ngapminder_df <- gapminder %>% as.data.frame()\n\n\n#tidy maken van data functie\ntidy_func <- function(path,name){\n  read_df <- read.csv(path, skip=11) %>% as.data.frame\n  col_nam <- colnames(read_df)\n  read_df %>% pivot_longer(cols=col_nam[-1], names_to=\"country\", values_to=paste0(\"activity\")) %>%\n    separate(Date, into = c(\"year\", \"month\",\"day\"),convert = TRUE) %>% mutate(\"disease\"=name)\n}\n\ndengue_tidy <- tidy_func(\"data/dengue_data.csv\",\"dengue\")\nflu_tidy <- tidy_func(\"data/flu_data.csv\",\"flu\")\n\n#opslaan als RDS\nsaveRDS(dengue_tidy, file = \"data/dengue_tidy.rds\")\nsaveRDS(flu_tidy, file = \"data/flu_tidy.rds\")\nsaveRDS(gapminder_df, file = \"data/gapminder.rds\")\n\n#opslaan als CSV\nwrite.csv(gapminder_df,\"data/gapminder.csv\")\nwrite.csv(dengue_tidy,\"data/dengue_tidy.csv\")\nwrite.csv(flu_tidy,\"data/flu_tidy.csv\")\n\ngapminder_df CREATE DATABASE workflows;\nlibrary(DBI)\n\n#Connectie opzetten\ncon <- dbConnect(RPostgres::Postgres(), \n                 dbname = \"workflows\", \n                 host=\"localhost\", \n                 port=\"5432\", \n                 user=\"postgres\",\n                 password=\"Datascience\")\n\n#Aanmaken van tabellen in databse\ndbWriteTable(con, \"flu\", flu_tidy)\ndbWriteTable(con, \"dengue\", dengue_tidy)\ndbWriteTable(con, \"gapminder\", gapminder_df)\n#Jaren verwijderen die niet aanwezig zijn in flu en dengue\ngapminder_minyear <- gapminder_df[gapminder_df$year >= min(flu_tidy$year), ]\ngapminder_maxyear <- gapminder_minyear[gapminder_minyear$year <= max(flu_tidy$year), ]\n\n#Landen weghalen die niet en flu en dengue aanwezig zijn\nlanden <- as.factor(c(dengue_tidy$country, flu_tidy$country,recursive=TRUE))\ngapminder_clean <- subset(gapminder_maxyear, country %in% landen)\n#Oude tabel verwijderen en nieuwe tabel invoegen\ndbRemoveTable(con, \"gapminder\")\ndbWriteTable(con, \"gapminder\", gapminder_clean)CREATE TABLE flu_dengue\nAS\n\nSELECT *\nFROM \n  flu \n\nUNION ALL\n \nSELECT *\n \nFROM\n  denguecreate table all_data\nas\n\nselect \ngapminder.*,\nflu_dengue.\"month\",\nflu_dengue.\"day\",\nflu_dengue.disease,\nflu_dengue.activity\n\nfrom gapminder \nleft join flu_dengue\non flu_dengue.\"year\" = gapminder.\"year\" and flu_dengue.country = gapminder.country\n    \nORDER BY gapminder.\"year\", gapminder.country, flu_dengue.\"month\", flu_dengue.\"day\"  ;\n#Data uit DB ophalen\nall_data <- dbReadTable(con, \"all_data\")\n#Dataset opslaan in data \nsaveRDS(all_data, file=\"data/all_data.rds\")\n#Connectie met database sluiten\ndbDisconnect(con) "},{"path":"databases.html","id":"data-analyse","chapter":"8 Databases","heading":"8.2 Data analyse","text":"Met de gecombineerde tabel worden een paar analyses uitgevoerdIn de code hierboven staat een functie die ervoor zorgt dat beide populatie en GDP van landen tegenover de gemiddelde search activity van flu en dengue van geplaatst wordt van een bepaald jaar. Dit gebeurt . Het leek mij immers interessant om te zien bij grotere populaties meer gezocht werd naar deze ziektes, omdat meer mensen voor meer spreiding zorgen, maar ook omdat meer mensen kunnen zoeken. Ook wou ik kijken naar het GDP om te kijken er verschil zit tussen rijkere en armere landen. Uit de functie volgt een plotly plot. Deze interactief en wanneer de muis de puntjes heen gaat zal het land en precieze data tevoorschijn komenFiguur3: Interactieve plot van google flu search activity 2008, met op de x-de activity en op de y-de GDP en populatie. De puntjes zijn per land gekleurd.het figuur hierboven wordt de flu activity bekeken. Het ziet er niet naar uit dat er een correlatie tussen populatie GDP en flu search activity. Canada heeft de meeste search activity terwijl niet een bepaald hoge lage populatie GDP. Het kan komen doordat de meeste mensen Canada Engels spreken en er dus automatisch naar “Flu” gezocht wordt. Kijkende naar Australia die beide een wat lagere en populatie en GDP dan Canada heeft daar tegen een veel lagere search activity. Maar twee datapunten zegt niet veel.Figuur4: Interactieve plot van google dengue search activity 2008, met op de x-de activity en op de y-de GDP en populatie. De puntjes zijn per land gekleurd.Net als bij flu ziet het eruit dat er geen correlatie tussen populatie GDP en activity.Bij de volgende analyse wordt er naar op welk tijdstip het jaar naar flu en dengue gezocht wordt om te kijken er dudielijke te zien wanneer het griep seizoen en dengue ook een seizoen heeft wanneer deze meer actief .Eerst wordt de data van beide dengue en flu de gemiddelde activity paar maand. Daarna er een functie die twee plots kan genereren.Wanneer er griep heerst zullen mensen dit sneller opzoeken op internet. Bij figuur … duidelijk te zien wanneer griep het meest heerst, wat vooral van september tot maart . Dit zakt ook weer de maanden van maart tot septemberBij dengue er een mindere sterke trend te zitten. Wel zijn er meer pieken te zien en het begin van het jaar en aan het einde, met vooral sterke pieken aan het einde. Bij de meeste jaren ziet het eruit dat het het midden van het jaar zakt, maar niet bij de jaren 2007,2009 en 2010.","code":"\nlibrary(plotly)\npopulatie_func <- function(dataset,year,name){\n  country_data <- dataset %>% filter(year==2008, disease==name) %>% \n    group_by(country) %>% \n    summarise(mean(activity, na.rm=TRUE), max(population),max(gdp))\n  colnames(country_data) <- c(\"country\",\"activity\",\"population\",\"GDP\")\n  country_data[order(country_data$population, decreasing=TRUE),]\n\n  plot1 <- plot_ly(data = country_data,\n                   x = ~activity,\n                   y = ~population, \n                   type = 'scatter',\n                   color =~country,\n                   showlegend=FALSE) %>%\n    layout(yaxis = list(title = 'Population'))\n\n  plot2 <- plot_ly(data = country_data,\n                   x = ~activity,\n                   y = ~GDP, \n                   type = 'scatter',\n                   color =~country,\n                   showlegend=FALSE) %>%\n    layout(xaxis = list(title = 'Activity'), \n           yaxis = list(title = 'GDP'))\n  \n  fig <- subplot(plot1,plot2, nrows = 2,titleY = TRUE, shareX = TRUE, titleX = TRUE) %>% \n    layout(title = list(text = paste(str_to_title(name),\"search activity tegen populatie en GDP in\", year)))\n  fig\n}\nlibrary(RColorBrewer)\nlibrary(ggplot2)\n#Summarise van data per year \nflu_dengue <- all_data %>% group_by(month, year, disease) %>% \n  na.omit() %>% filter(year!=2002) %>% \n  summarise(mean(activity))\ncolnames(flu_dengue) <- c(\"month\",\"year\",\"disease\", \"activity\")\n\n#Functie voor plot van verloop activity\nactivity_years_func <- function(dataset,name){\n  dataset %>% filter(disease==name) %>%\n    ggplot(aes(x=month,y=activity, group=year, color=factor(year)))+\n    geom_line()+\n    scale_x_continuous(name=\"Month\", breaks=(1:12), limits=c(1, 12))+\n    scale_color_brewer(palette = \"Set1\")+\n    theme_minimal()\n}\nactivity_years_func(flu_dengue,\"flu\")\nactivity_years_func(flu_dengue,\"dengue\")"},{"path":"databases.html","id":"bronnen","chapter":"8 Databases","heading":"8.3 Bronnen","text":"Bronnen:\nFlu dataset: “Data Source: Google Flu Trends (http://www.google.org/flutrends)”Dengue dataset: “Data Source: Google Dengue Trends (http://www.google.org/denguetrends)”","code":""},{"path":"r-packages.html","id":"r-packages","chapter":"9 R packages","heading":"9 R packages","text":"Packages zijn een manier om je data en functies gebruikt bepaalde onderzoeken en analyses op te slaan en later weer op te halen. Door documentatie toe te voegen zijn de functies en data weer makkelijk te gebruiken voor mensen die jouw onderzoek willen reproduceren.Voor dit portfolio heb ik een package aangemaakt met de functies die hier gebruikt worden.","code":""},{"path":"r-packages.html","id":"installeren-van-package","chapter":"9 R packages","heading":"9.1 Installeren van package","text":"Installeer devtools eerst:De package zelf kan geïnstalleerd worden van de github repo:","code":"\ninstall.packages(\"devtools\")\ndevtools::install_github(\"lisa-kuipers26/LisasParket\",build_vignettes = TRUE)"},{"path":"r-packages.html","id":"package-gebruiken","chapter":"9 R packages","heading":"9.2 Package gebruiken","text":"Gebruik library om de package te laden","code":"\nlibrary(LisasParket)"},{"path":"r-packages.html","id":"functies","chapter":"9 R packages","heading":"9.3 Functies","text":"De functies aanwezig de package zijn:activity_year_funcpopulation_functidy_funccovid19_progressplotDe eerste 3 functies zijn te zien databases tab en de laatste bij de parameteres tab.Nu kunnen die functies gebruikt worden, hieronder een voorbeeld:Omdat er documentatie toegevoegd bij de functies, het mogelijk om de help te raadplegen door bijvoorbeeld ?tidy_func(). Hieronder de outpout van de help pagina.","code":"\ntidy_func(\"data/dengue_data.csv\",\"dengue\") %>% \n  head() %>%\n  kbl() %>%\n  kable_styling(bootstrap_options = \"striped\", \n                full_width = F, \n                position = \"left\")"},{"path":"r-packages.html","id":"data","chapter":"9 R packages","heading":"9.4 Data","text":"Naast functie kunnen ook datasets opgeslagen worden. deze package heb ik voor demonstratie de gapminder gefiltered en opgeslagen als een nieuwe dataset. Om de dataset te laden gebruik simpelweg data(\"datset\"). Om te kijken wat voor data een package bevat gebruik `data(package=“LisasParket”).","code":"\ndata(\"gapminder_netherlands\")\ngapminder_netherlands %>%\n  kbl() %>%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\"), \n                full_width = F) %>%\n  scroll_box(width = \"100%\")"},{"path":"r-packages.html","id":"vignette","chapter":"9 R packages","heading":"9.5 Vignette","text":"Door browseVignette(\"LisasParket\") te gebruiken wordt een vignette geopend waarin informatie het pakket staat. De informatie aanwezig het vignette komt overeen met wat ik net heb laten zien.","code":""},{"path":"parameters-voor-flexibiliteit.html","id":"parameters-voor-flexibiliteit","chapter":"10 Parameters voor flexibiliteit","heading":"10 Parameters voor flexibiliteit","text":"Parameters yaml headers kunnen gebruikt worden om je code dynamischer te maken. Het een snelle manier van variabelen veranderen bij het filteren van een dataset.Hieronder staat een voorbeeld van hoe de yaml header eruit ziet met verschillende parameters. Voor de analyse verderop hebben dezelfde paramteres gebruitk als de afbeelding.\nFigure 10.1: Afbeelding 1: Parameters de yaml header.\nDe parameters kunnen een interactief scherm weergegeven worden. Deze verschijnd wanneer het bestand geknit wordt.\nFigure 10.2: Afbeelding 2: Parameters bij knitten.\nHieronder staat de code van het filteren van de de EDCC_daily dataset gevonden op: https://www.ecdc.europa.eu/en/covid-19/data. de dataset staan het aantal gevallen en sterfgevallen van COVID-19 uit de periode van 2020 t/m 2022. De parameters die boven staan zijn gebruikt het filteren. Wanneer iemand van dezelfde jaar, maand dag de data wilt weergeven kan bij de end en start parameters dezelfde datum ingevoerd worden. Ik heb ervoor gekozen om de hele beschikbare periode te nemen voor deze analyse, om te kijken er een verband tussen de cases deaths.de code te zien dat de dataset gefilterd wordt met de parameters door params te gebruiken voor de gedefineerde parameter naam uit de yaml header.Figuur7: Een interactieve plot van het aantal cases en deaths een tijd periode van 2020 t/m 2022 met het aantal deaths en cases op de y-en de datum op de x-.Uit de gefilterde set en functie zijn twee grafieken gegenereerd. Deze zijn smen gevoegd met plotly waardoor de x-gelijk loopt en het mogelijk om op bepaalde stukken te zoomen. Wat ik interessant vind om te zien bij deze figuren dat het aantal doden aan het begin het hoogste lag terwijl het aantal cases relatief niet heel hoog waren. Bij de eerste piek van cases bij het aantal dode ook een piek te zien ronde dezelfde datum. Daarna zwakt het aantal doden vergeleken met de cases af. dit komt doordat mensen met een verzwakt imuunsysteem sneller zijn overleden omdat vaccinaties hun werk gedaan hebben niet direct op te maken uit deze data, maar het wel een interessant verband.Brivio, Erica, Franco Locatelli, Marta Lopez-Yurda, Andrea Malone, Cristina Díaz-de-Heredia, Bella Bielorai, Claudia Rossig, et al. 2021. “Phase 1 Study Inotuzumab Ozogamicin Pediatric Relapsed/Refractory Acute Lymphoblastic Leukemia (ITCC-059 Study).” Blood 137 (12): 1582–90. https://doi.org/10.1182/blood.2020007848.“High-throughput Screening.” n.d. Research. Accessed June 11, 2022. https://research.prinsesmaximacentrum.nl/nl/kernfaciliteiten/high-throughput-screening.Liu, Bailing, Songjun Li, Jie Hu. 2004. “Technological Advances High-Throughput Screening.” American Journal Pharmacogenomics 4 (4): 263–76. https://doi.org/10.2165/00129785-200404040-00006.Mayr, Lorenz M, Dejan Bojanic. 2009. “Novel Trends High-Throughput Screening.” Current Opinion Pharmacology, Anti-infectives/New technologies, 9 (5): 580–88. https://doi.org/10.1016/j.coph.2009.08.004.Wander, Priscilla, Susan T. C. J. M. Arentsen-Peters, Sandra S. Pinhanҫos, Bianca Koopmans, M. Emmy M. Dolman, Rijndert Ariese, Frank L. Bos, et al. 2021. “High-Throughput Drug Screening Reveals Pyrvinium Pamoate Effective Candidate Pediatric MLL-Rearranged Acute Myeloid Leukemia.” Translational Oncology 14 (5): 101048. https://doi.org/10.1016/j.tranon.2021.101048.","code":"\nlibrary(tidyverse)\nlibrary(plotly)\nlibrary(params)\nlibrary(stringr)\n\n#Data inlezen\ndata_table <- read_csv(\"data/EDCC_daily.csv\")\n\n#Data filteren\ndata_filtered <- data_table %>% filter(year >= params$start_year & year <= params$end_year & countriesAndTerritories == params$country & day %in% c(params$start_day:params$end_day) & month %in% c(params$start_month:params$end_month))\n\n#Oploopbare range maken\ndata_filtered$date_range <- paste0(data_filtered$year,sprintf(\"%02d\",data_filtered$month),sprintf(\"%02d\",data_filtered$day)) \n\n#Functie voor plot\ncovid19_progressplot <- function(dataset,condition){\n  plot_ly(data = dataset,\n                   x = ~date_range,\n                   y = ~dataset[[condition]], \n                   type = 'scatter',\n                   showlegend=FALSE) %>%\n    layout(xaxis=list(title=\"Date\"),\n           yaxis=list(title=condition))\n}\nplot1 <- covid19_progressplot(data_filtered,\"cases\")\nplot2 <- covid19_progressplot(data_filtered,\"deaths\")\n\nsubplot(plot1,plot2, nrows = 2,titleY = TRUE, shareX = TRUE, titleX = TRUE) %>%\n  layout(title=list(text = paste(\"COVID-19 cases and deaths from\",\n                                   data_filtered$dateRep[length(data_filtered$dateRep)],\n                                   \"to\",\n                                   data_filtered$dateRep[1],\n                                   \"in Belgium\")))"}]
