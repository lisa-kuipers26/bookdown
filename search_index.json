[["index.html", "Portofolio data science 1 Introductie", " Portofolio data science Lisa Kuipers 2022-06-27 1 Introductie In dit portfolio staan verschillende onderwerpen die over reproduceerbaar werken en goede workflows gaan. Voor elk onderwerp is een aparte pagina gemaakt. Op deze pagina’s staat beschreven wat ik geleerd heb tijdens mijn minor data science. "],["verwerken-van-data.html", "2 Verwerken van data", " 2 Verwerken van data Het interpreteren en verwerken van data van iemand anders is belangrijk in de data science. Er wordt hier data gebruikt van een onderzoek waar C. elegans nematoden blootgesteld aan verschillende soorten stoffen. Belangrijkste in een data analyse is bergrijpen waar je data over gaat. Door de metadata te lezen krijg je een goed idee van waar je dataset overgaat en wat elke kopje betekent. Voor dit onderzoek willen we het effect weten van verschillende stoffen op de nematode maar ook de het effect van verschillende concentraties. Om dit te onderzoeken worden de kolommen RawData, compConcentration en expType gebruikt. In RawData staan het aantal nematodes op de plaat, in de compConcentration de concentratie van de stof en de expType de soort stof. Om globaal een idee te krijgen van het effect worden deze variabel tegen elkaar uitgezet in de grafiek hieronder. library(DT) library(kableExtra) library(readxl) library(tidyverse) tabel &lt;- read_excel(here::here(&quot;data/CE.LIQ.FLOW.062_Tidydata.xlsx&quot;)) tabel$compConcentration &lt;- as.double(tabel$compConcentration) tabel$compName &lt;- as.factor(tabel$compName) #x-as concentreatie in -10log tabel$compConcentration &lt;- ifelse(tabel$compConcentration==0,0,-log10(tabel$compConcentration)) tabel %&gt;% ggplot(aes(x=compConcentration,y=RawData,color=compName,shape=expType))+ geom_jitter(width=0.1)+ labs(x=&quot;Concentratie (-log10) &quot;, y=&quot;Number of offspring&quot;) In het experiment is de positieve controle van dit experiment is ethanol . De negatieve controle van dit experiment is de S-medium Bij het verwerken van data is het belangrijk dat de data goed ingelezen wordt. Eén ding waar we tegen lopen bij deze dataset is dat de compConcentration ingelezen wordt als character type i.p.v. een getal. Hierdoor klopt de x-as niet met wat je wilt zien, omdat er groepen en geen schaal weergegeven wordt. Dit wordt opgelost door het typ evan compConcentration te veranderen van een character naar een double. De bovenstaande grafiek geeft een globaal overzicht, maar er kunnen geen conclusies uitgetrokken worden. Hiervoor is een verdere data analyse nodig. Zo’n data analyse kan er als volgt uitzien: Data plotten zodat er een globaal overzicht hebt. Nul hypothese voor shapiro wilks test (Data is normaal verdeeld) Bij &gt;0,05 nulhypothese wordt aangenomen, de data is normaal verdeeld. &lt;0,05 nul-hypothese wordt verworpen Bij een normaal verdeelde dataset kan er een pearson test uitgevoerd worden. Nulhypothese van pearson test is dat er geen verband is tussen de concentratie en aantal nematode op de plaat. Nulhypothese &lt;0,05 nulhypothese wordt verworpen, er is een correlatie tussen de concentratie en aantal nematode op de plaat. &gt;0,05 Er is geen verband Natuurlijk wordt er bij zes alleen een berekening gedaan en er kunnen natuurlijk ook andere factoren geweest zijn die de uitslag beïnvloed hebben. #Genormalizeerd en gemiddelde naar 1 neg_controle &lt;- tabel %&gt;% filter(expType==&quot;controlNegative&quot;) tabel$RawData &lt;- tabel$RawData / mean(neg_controle$RawData) tabel %&gt;% ggplot(aes(x=compConcentration,y=RawData,color=compName,shape=expType))+ geom_jitter(width=0.1)+ labs(x=&quot;Concentratie (-log10) &quot;, y=&quot;Number of offspring&quot;) Bij de grafiek hierboven is de dataset genormaliseerd door de RawData te delen daar het gemiddelde van de negatieve controle. Dit is gedaan omdat in de concentratie en aantal nematode een grote range zit. Op deze manier kunnen hogere getallen de uitslag van analyses onterecht beïnvloeden. Door het op deze manier te normaliseren wordt de range kleiner en is er minder bias. "],["open-peer-review.html", "3 Open Peer Review 3.1 Onderzoek op reproduceerbaarheid beoordelen 3.2 Code zelf reproduceren", " 3 Open Peer Review 3.1 Onderzoek op reproduceerbaarheid beoordelen In dit gedeelte wordt een artikel beoordeeld op de reproduceerbaarheid. Deze wordt beoordeeld aan de hand van verschillende criteria omschreven in de onderstaande tabel. Voor de beoordeling is een artikel gekozen primair onderzoek dat beschikbaar is op PMC. Gebruikte artikel: Amawi KF, Alkhatib AJ. Urtica Pilulifera in Treating Pre-diabetic Rat Model to Control the Blood Glucose, Lipids and Oxidative Stress. Med Arch. 2020;74(3):168-171. doi:10.5455/medarh.2020.74.168-171 https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7405998/ 3.1.1 Omschrijving onderzoek Het doel van het onderzoek is kijken of Urtica pilulifera (een plant) effect heeft op pre-diabetische ratten en ook de antioxiderende werking onderzoeken De ratten werden ingedeeld in drie groepen van 10; een controle groep, een pre diabetische groep, en een groep met de behandeling van U. pilulifera. De ratten pre-diabetisch gemaakt kregen een hoog sucrose dieet, de controle groep een normaal dieet en de behandelde groep kreeg hetzelfde dieet als de pre-diabetische ratten met U. pilulifera extract geïnjecteerd. Na 30 dagen werden bloed samples afgenomen en getest op glucose, triglycerides, cholesterol, GSH, TAC en MDA Uit het onderoek bleek dat glucose, triglyceride en MDA niveaus in de pre-diabetic groep significant verhoogd waren en significant verlaagd in de U. pilulifera groep. GSH en TAC was significant hoger in de U. pilulifera ten opzichte van de pre-diabetische groep. Er zat geen significant verschil in cholesterol niveau in de groepen. 3.1.2 Beoordeling artikel In de tabel staat of het artikel aan de verschillende criterium voldoet. Criteria Answer Study purpose Yes Data availability No Data location Data location not stated Study location Yes Author review Author listed but did not fill out contact Ethics statement No Funding statement Yes Code availability No Het blijkt dat het artikel maar aan drie van acht criteria voldoet. Hoewel het artikel goed te lezen was is deze dus toch niet goed reproduceerbaar. 3.2 Code zelf reproduceren Net is er gekeken reporduceerbaarheid van een primair onderzoek. In dit onderdeel wordt er gekeken naar de reporduceerbaarheid van (R) code. Data en code van onderstaand onderzoek gebruikt: Strozza C, Myrskylä M. Monitoring trends and differences in COVID-19 case-fatality rates using decomposition methods: Contributions of age structure and age-specific fatality. PLoS One. 2020 Sep 10;15(9):e0238904. doi: 10.1371/journal.pone.0238904. PMID: 32913365; PMCID: PMC7482960. https://pubmed.ncbi.nlm.nih.gov/32913365/ Link naar de code: https://osf.io/g7vjd/ 3.2.1 Beoordeling script Het script zelf was goed te lezen. Er waren 4 scripts die elkaar opvolgde, door de getallen in de bestandsnaam was het makkelijk te zien welke als eerst uitgevoerd moest worden. In de scripts zelf stonden genoeg comments om te begrijpen waar elk stuk code voor diende en ziet de code zelf er netjes uit. Ik geef de leesbaarheid dus ook een 5/5 De code was ook goed reproduceerbaar. Er was alleen één probleem wat handmatig opgelost moest worden, dit probleem en hoe deze opgelost is staat beschreven hieronder. Hierdoor krijgt de reproduceerbaarheid een 4/5, want nadat dit probleem opgelost was, konden alle tabellen makkelijk gemaakt worden. 3.2.2 Probleem in script ### Monitoring trends and differences in COVID-19 case fatality ############## ### rates using decomposition methods: A demographic perspective ############## ### Last updated: 2020-07-16 09:27:20 CEST ### Contact: ### riffe@demogr.mpg.de ### acosta@demogr.mpg.de ### dudel@demogr.mpg.de ### Get data ################################################################## # Required packages source((&quot;R/00_functions.R&quot;)) # URL + filename url &lt;- &#39;https://osf.io/wu5ve//?action=download&#39; filename &lt;- &#39;Data/Output_10.csv&#39; # Load data GET(url, write_disk(filename, overwrite = TRUE)) dat &lt;- read_csv(filename,skip=3) Bij het uitvoeren van de code hierboven kwam de onderstaande foutmelding: (#fig:error_r)Error in R In de foutmelding is een html bestand te zien. Dit hoort natuurlijk niet bij een read_csv() functie. In de code wordt het bestand van een online directory gedownload. De URL staat in de code en wanneer deze bezocht werd stond er dit bericht: (#fig:error_web)Error op de webpagina Het bestand bevindt zich niet meer op de plek waar de URL naar verwees en kon dus ook niet gedownload worden waardoor de html van de pagina werd overgenomen. Er was echter wel een menu aan de zijkant aanwezig waar het mogelijk is om bij de github van het project te komen. Hier vond ik het output bestand wat nodig is voor de code en heb deze handmatig gedownload en toegevoegd aan de /data directory. Nadat deze in de directory stond kon het script en de rest van de scripts vlekkeloos uitgevoerd worden. Hieronder staat het script met de aanpassing. De rest van het script kan gevonden worden op de bovenstaande link. De output van het script zijn 6 verschillende excel bestanden met data over het verloop van COVID-19 in 6 verschillende landen. ### Monitoring trends and differences in COVID-19 case fatality ############## ### rates using decomposition methods: A demographic perspective ############## ### Last updated: 2020-07-16 09:27:20 CEST ### Contact: ### riffe@demogr.mpg.de ### acosta@demogr.mpg.de ### dudel@demogr.mpg.de ### Get data ################################################################## # Required packages source((&quot;R/00_functions.R&quot;)) # URL + filename filename &lt;- &#39;data/Output_10.csv&#39; # Load data dat &lt;- read_csv(filename,skip=3) ### Edit data (select countries, etc.) ######################################## # Lists of countries and regions countrylist &lt;- c(&quot;China&quot;,&quot;Germany&quot;,&quot;Italy&quot;,&quot;South Korea&quot;,&quot;Spain&quot;,&quot;USA&quot;) region &lt;- c(&quot;All&quot;,&quot;NYC&quot;) # Restrict dat &lt;- dat %&gt;% filter(Country %in% countrylist &amp; Region %in% region) # Remove Tests variable dat &lt;- dat %&gt;% mutate(Tests=NULL) # Drop if no cases/Deaths dat &lt;- na.omit(dat) ### Save ###################################################################### write_csv(dat,file=&quot;Data/inputdata.csv&quot;) "],["data-organiseren.html", "4 Data organiseren", " 4 Data organiseren Het organizeren van data is belangrijk voor reproduceerbaarheid en algemene goede workflow. Hieronder staat een voorbeeld van een map met verschillende projecten. Elke map heeft dezelfde indeling, bij de data en scripts zijn README bestanden toegevoegd waarin metadata staat over het project en de bestanden. Ik vind het zelf fijn dat wanneer er veel van één soort bestand is (zoals bij bam en fastq bestanden) dat deze in een aparte map staan. De rest bestanden staan gewoon in de /data. ## data/daur2 ## ├── metagenomics_formatief ## │ ├── README.md ## │ ├── data ## │ │ ├── HU_waternet_MOCK2_composition.csv ## │ │ └── README.md ## │ ├── metagenomics_formatief.Rmd ## │ ├── metagenomics_formatief.html ## │ └── metagenomics_reader.Rproj ## ├── metagenomics_reader ## │ ├── README.md ## │ ├── data ## │ │ ├── HU_waternet_MOCK1_composition.csv ## │ │ └── README.md ## │ ├── metagenomics_reader.Rmd ## │ ├── metagenomics_reader.Rproj ## │ └── metagenomics_reader.html ## ├── rna_seq_airway ## │ ├── README.md ## │ ├── data ## │ │ ├── README.md ## │ │ └── ipsc_sampledata.csv ## │ ├── rnaseq_airway.Rmd ## │ ├── rnaseq_airway.Rproj ## │ └── rnaseq_airway.html ## ├── rna_seq_ipsc ## │ ├── README.md ## │ ├── data ## │ │ ├── README.md ## │ │ └── ipsc_sampledata.csv ## │ ├── rnaseq_ipsc.Rmd ## │ ├── rnaseq_ipsc.Rproj ## │ └── rnaseq_ipsc.html ## └── rna_seq_onecut ## ├── README.md ## ├── data ## │ ├── README.md ## │ └── onecut_sampledata_OC2.csv ## ├── rnaseq_onecut.Rmd ## ├── rnaseq_onecut.Rproj ## └── rnaseq_onecut.html Hieronder staat een voorbeeld van de readme die ik in de repo van dit portfolioo heb staan. Ik beschrijf kort wat er in de datasets staat en waarvoor ze gebruikt. Ook staat er waar ik ze vandaan gehaald heb. (#fig:readme voorbeeld)README voorbeeld bij data bestanden "],["cv.html", "5 cv", " 5 cv Lisa Kuipers Figure 5.1: Foto van mij Contact 0612345678 lisa.kuipers26@hotmail.com Werkervaring HEMA Rhenen | 2015 – heden Kassa medewerker, horeca medewerker Klanten helpen in de winkel, producten afrekenen en helpen op de klantenservice Klanten bedienen in de horeca Aanvullen van producten in de winkel Opleiding Rembrandt college Veenendaal | 2012 - 2017 HAVO diploma gehaald met het profiel natuur en gezondheid Hogeschool Utrecht | 2019 – Heden HBO Biologie en medisch laboratoriumonderzoek (life science) Specialisatie biomolecular research Minor data science for biology Talen Nederlands Engels Vaardigheden Microsoft office R Bash Python SQL "],["het-citeren-van-andere-werken.html", "6 Het citeren van andere werken", " 6 Het citeren van andere werken Het citeren van werken van is belangrijk om fraude tegen te gaan, maar ook om je uitspraken meer geloofwaardig te laten overkomen. Hieronder staat een stuk tekst waar naar verschillende bronnen verwezen wordt. Het maxima center in Utrecht houdt zich bezig met het onderzoeken en behandelen van kinderoncologie. Zo worden er verschillende technieken ontwikkeld en getest om een zo goed mogelijke behandeling te geven aan een patiënt. Zo was er in een onderzoek naar leukemie in kinderen een antilichaam gebruikt die DNA bindingen kan verbreken en de kankercellen dus dood gaan.(Brivio et al. 2021). Een andere techniek die gebruikt wordt is high-throughput screening (HTS). HTS is een techniek waarbij er een groot aantal stoffen getest in één keer getest kunnen worden. Dit levert grote hoeveelheden data op, en de kans van het vinden van een effectief medicijn wordt vergroot. Samen met verschillende fluorescentie technieken en kunnen er tot wel 100000 samples gescreend worden per dag (Liu, Li, and Hu 2004). Hoewel er in het begin van de ontwikkeling van de HTS techniek vooral gezorgd werd om het aantal assays omhoog te krijgen, wordt er tegenwoordig vooral gekeken naar assays met meer fysiologische relevantie zodat er minder tests gedaan hoeven te worden en kosten lager kunnen worden(Mayr and Bojanic 2009). Het prinses maxima center hebruikte HTS onder andere om een effectieve drug te vinden tegen acute myeloïde leukemie. De drug Pyrvinium pamoate bereikte een IC50 bij concentratie onder de 80 nM, en reduceerde cel groei en deling met 50% (Wander et al. 2021). Om HTS mogelijk te maken wordt bij het prinses maxima wordt gebruik gemaakt van het Beckman Coulter Biomek i7 Hybrid werkstation, waar het voorbereiden van de platen plaats vindt en er in grote snel- en nauwkeurigheid een 384 wells plaat gevuld kan worden (“High-throughput Screening” n.d.). In de wells zitten verschillende soorten medicatie of controles, Het bijhouden van wat er in welke well zit werd tot op het heden met de hand in Excel gemaakt. Het maxima center vond dat dit makkelijker kon en heeft ons de opdracht gegeven om een app te maken waar deze informatie sneller en nauwkeuriger ingevoerd kan worden. Door gebruik te kunnen maken van de programmeertaal R wordt de app gemaakt met de shiny package. In de app zelf is het mogelijk om layouts up te loaden en deze aan te passen naar de huidige opzet. Zo kan de onder andere de drug, concentratie en de aanwezigheid van cellen aangepast worden. Met visualisatie plots is het mogelijk om deze aanpassing te zien en door de keuzes standaard te maken, worden fouten sneller voorkomen. Nadat alle aanpassing gemaakt zijn kan het bestand gedownload worden en staat de nieuwe data in een .CSV bestand. Bibliografie Brivio, Erica, Franco Locatelli, Marta Lopez-Yurda, Andrea Malone, Cristina Díaz-de-Heredia, Bella Bielorai, Claudia Rossig, et al. 2021. “A Phase 1 Study of Inotuzumab Ozogamicin in Pediatric Relapsed/Refractory Acute Lymphoblastic Leukemia (ITCC-059 Study).” Blood 137 (12): 1582–90. https://doi.org/10.1182/blood.2020007848. “High-throughput Screening.” n.d. Research. Accessed June 11, 2022. https://research.prinsesmaximacentrum.nl/nl/kernfaciliteiten/high-throughput-screening. Liu, Bailing, Songjun Li, and Jie Hu. 2004. “Technological Advances in High-Throughput Screening.” American Journal of Pharmacogenomics 4 (4): 263–76. https://doi.org/10.2165/00129785-200404040-00006. Mayr, Lorenz M, and Dejan Bojanic. 2009. “Novel Trends in High-Throughput Screening.” Current Opinion in Pharmacology, Anti-infectives/New technologies, 9 (5): 580–88. https://doi.org/10.1016/j.coph.2009.08.004. Wander, Priscilla, Susan T. C. J. M. Arentsen-Peters, Sandra S. Pinhanҫos, Bianca Koopmans, M. Emmy M. Dolman, Rijndert Ariese, Frank L. Bos, et al. 2021. “High-Throughput Drug Screening Reveals Pyrvinium Pamoate as Effective Candidate Against Pediatric MLL-Rearranged Acute Myeloid Leukemia.” Translational Oncology 14 (5): 101048. https://doi.org/10.1016/j.tranon.2021.101048. "],["databases.html", "7 Databases", " 7 Databases Om wat verschillende functies van de database te laten zien worden er verschillende datasets ingeladen. Hieronder wordt de data van de dengue en flu google trends tidy gemaakt, door de landnamen in één kolom te zetten. Ook wordt er een aparte kolom gemaakt met de naam van de ziekte. Voorderest worden ook de datum opgesplitst in dagen, maanden en jaren zodat deze later samengevoegd kunnen worden met de gapminder data. library(dslabs) library(tidyverse) #laden van gapminder gapminder_df &lt;- gapminder %&gt;% as.data.frame() #tidy maken van data functie tidy_func &lt;- function(path,name){ read_df &lt;- read.csv(path, skip=11) %&gt;% as.data.frame col_nam &lt;- colnames(read_df) read_df %&gt;% pivot_longer(cols=col_nam[-1], names_to=&quot;country&quot;, values_to=paste0(&quot;activity&quot;)) %&gt;% separate(Date, into = c(&quot;year&quot;, &quot;month&quot;,&quot;day&quot;),convert = TRUE) %&gt;% mutate(&quot;disease&quot;=name) } dengue_tidy &lt;- tidy_func(&quot;data/dengue_data.csv&quot;,&quot;dengue&quot;) flu_tidy &lt;- tidy_func(&quot;data/flu_data.csv&quot;,&quot;flu&quot;) flu_tidy %&gt;% filter(year==2005) #opslaan als RDS saveRDS(dengue_tidy, file = &quot;data/dengue_tidy.rds&quot;) saveRDS(flu_tidy, file = &quot;data/flu_tidy.rds&quot;) saveRDS(gapminder_df, file = &quot;data/gapminder.rds&quot;) #opslaan als CSV write.csv(gapminder_df,&quot;data/gapminder.csv&quot;) write.csv(dengue_tidy,&quot;data/dengue_tidy.csv&quot;) write.csv(flu_tidy,&quot;data/flu_tidy.csv&quot;) Er moet een database gemaakt worden om de tabellen op te slaan. Hieronder staat SQL code die ingevoerd kan worden bij het programma dBeaver (of een andere database software) voor het creëren van een database. CREATE DATABASE workflows; Om de database te gebruiken in R wordt er een connectie aangemaakt. Met de connectie kan R code gebruikt worden voor interactie met de database. Dit is handig omdat er dan niet steeds tussen twee programma’s hoeft te switchen. Via de connectie worden de verschillende tabellen ingeladen in de database. library(DBI) #Connectie opzetten con &lt;- dbConnect(RPostgres::Postgres(), dbname = &quot;workflows&quot;, host=&quot;localhost&quot;, port=&quot;5432&quot;, user=&quot;postgres&quot;, password=&quot;Datascience&quot;) #Aanmaken van tabellen in databse dbWriteTable(con, &quot;flu&quot;, flu_tidy) dbWriteTable(con, &quot;dengue&quot;, dengue_tidy) dbWriteTable(con, &quot;gapminder&quot;, gapminder_df) B bBññn ñ Om de gapminder wat netter te maken heb ik ervoor gekozen om alvast de landen en jaren die niet in de flu en dengue google trend data voorkomt te verwijderen. #Jaren verwijderen die niet aanwezig zijn in flu en dengue gapminder_minyear &lt;- gapminder_df[gapminder_df$year &gt;= min(flu_tidy$year), ] gapminder_maxyear &lt;- gapminder_minyear[gapminder_minyear$year &lt;= max(flu_tidy$year), ] #Landen weghalen die niet en flu en dengue aanwezig zijn landen &lt;- c(dengue_tidy$country, flu_tidy$country,recursive=TRUE) gapminder_clean &lt;- subset(gapminder_maxyear, country %in% landen) De oude tabel gapminder wordt via de connectie verwijderd en de nette wordt ingevoerd #Oude tabel verwijderen en nieuwe tabel invoegen dbRemoveTable(con, &quot;gapminder&quot;) dbWriteTable(con, &quot;gapminder&quot;, gapminder_clean) CREATE TABLE flu_dengue AS SELECT * FROM flu UNION ALL SELECT * FROM dengue create table all_data as select gapminder.*, flu_dengue.&quot;month&quot;, flu_dengue.&quot;day&quot;, flu_dengue.disease, flu_dengue.activity from gapminder left join flu_dengue on flu_dengue.&quot;year&quot; = gapminder.&quot;year&quot; and flu_dengue.country = gapminder.country ORDER BY gapminder.&quot;year&quot;, gapminder.country, flu_dengue.&quot;month&quot;, flu_dengue.&quot;day&quot; ; #Data uit DB ophalen all_data &lt;- dbReadTable(con, &quot;all_data&quot;) #Dataset opslaan in data saveRDS(all_data, file=&quot;data/all_data.rds&quot;) #Connectie met database sluiten dbDisconnect(con) #Data laden voor analyse all_data &lt;- readRDS(&quot;data/all_data.rds&quot;) library(gridExtra) populatie_func &lt;- function(dataset,year,name){ country_data &lt;- dataset %&gt;% filter(year==2008, disease==name) %&gt;% group_by(country) %&gt;% summarise(mean(activity, na.rm=TRUE), max(population),max(gdp)) colnames(country_data) &lt;- c(&quot;country&quot;,&quot;activity&quot;,&quot;population&quot;,&quot;GDP&quot;) country_data[order(country_data$population, decreasing=TRUE),] plot1 &lt;- country_data %&gt;% ggplot(aes(x=country, y=population))+ geom_col()+ theme_minimal()+ theme(axis.text.x = element_text(angle = 90,vjust=0.3,hjust=1)) plot2 &lt;- country_data %&gt;% ggplot(aes(x=country, y=GDP))+ geom_col()+ theme_minimal()+ theme(axis.text.x = element_text(angle = 90,vjust=0.3,hjust=1)) plot3 &lt;- country_data %&gt;% ggplot(aes(x=country, y=activity))+ geom_col()+ theme_minimal()+ theme(axis.text.x = element_text(angle = 90,vjust=0.3,hjust=1)) grid.arrange(plot1,plot2,plot3) } populatie_func(all_data, 2008, &quot;flu&quot;) populatie_func(all_data, 2008, &quot;dengue&quot;) library(RColorBrewer) #Summarise van data per year flu_dengue &lt;- all_data %&gt;% group_by(month, year, disease) %&gt;% na.omit() %&gt;% filter(year!=2002) %&gt;% summarise(mean(activity)) colnames(flu_dengue) &lt;- c(&quot;month&quot;,&quot;year&quot;,&quot;disease&quot;, &quot;activity&quot;) #Functie voor plot van verloop activity activity_years_func &lt;- function(dataset,name){ dataset %&gt;% filter(disease==name) %&gt;% ggplot(aes(x=month,y=activity, group=year, color=factor(year)))+ geom_line()+ scale_x_continuous(name=&quot;Month&quot;, breaks=(1:12), limits=c(1, 12))+ scale_color_brewer(palette = &quot;Set1&quot;)+ theme_minimal() } activity_years_func(flu_dengue,&quot;flu&quot;) activity_years_func(flu_dengue,&quot;dengue&quot;) "],["r-packages.html", "8 R packages", " 8 R packages "],["parameters-voor-flexibiliteit.html", "9 Parameters voor flexibiliteit", " 9 Parameters voor flexibiliteit Parameters in yaml headers kunnen gebruikt worden om je code dynamischer te maken. Het is een snelle manier van variabelen veranderen bij het filteren van een dataset. Hieronder staat een voorbeeld van hoe de yaml header eruit ziet met verschillende parameters. Voor de analyse verderop hebben we dezelfde paramteres gebruitk als in de afbeelding. Figure 9.1: Afbeelding 1: Parameters in de yaml header. De parameters kunnen in een interactief scherm weergegeven worden. Deze verschijnd wanneer het bestand geknit wordt. Figure 9.2: Afbeelding 2: Parameters bij knitten. Hieronder staat de code van het filteren van de de EDCC_daily dataset gevonden op: https://www.ecdc.europa.eu/en/covid-19/data. In de dataset staan het aantal gevallen en sterfgevallen van COVID-19 uit de periode van 2020 t/m 2022. De parameters die boven staan zijn gebruikt in het filteren. Wanneer iemand van dezelfde jaar, maand of dag de data wilt weergeven kan bij de end en start parameters dezelfde datum ingevoerd worden. Ik heb ervoor gekozen om de hele beschikbare periode te nemen voor deze analyse, om te kijken of er een verband is tussen de cases and deaths. library(tidyverse) library(ggplot2) library(params) library(stringr) #Data inlezen data_table &lt;- read_csv(&quot;data/EDCC_daily.csv&quot;) #Data filteren data_filtered &lt;- data_table %&gt;% filter(year &gt;= params$start_year &amp; year &lt;= params$end_year &amp; countriesAndTerritories == params$country &amp; day %in% c(params$start_day:params$end_day) &amp; month %in% c(params$start_month:params$end_month)) #Oploopbare range maken data_filtered$date_range &lt;- paste0(data_filtered$year,sprintf(&quot;%02d&quot;,data_filtered$month),sprintf(&quot;%02d&quot;,data_filtered$day)) #Functie voor plot plot_func &lt;- function(dataset,condition){ dataset %&gt;% ggplot(aes(x=date_range, y=dataset[[condition]]))+ geom_point()+ scale_x_discrete(breaks=data_filtered$date_range[seq(1,length(data_filtered$date_range),by=50)])+ theme(axis.text.x = element_text(angle = 90))+ labs(x=&quot;Date&quot;, y=condition, title = paste(&quot;Amount of COVID-19&quot;,condition,&quot;from&quot;,data_filtered$dateRep[length(data_filtered$dateRep)],&quot;to&quot;,data_filtered$dateRep[1],&quot;in Belgium&quot;)) } In de code is te zien dat de dataset gefilterd wordt met de parameters door params te gebruiken voor de gedefineerde parameter naam uit de yaml header. plot_func(data_filtered,&quot;cases&quot;) (#fig:functie uitvoeren cases)Figuur 1: Aantal COVID-19 cases van 01/03/2020 t/m 23/05/2022 in belgië. Aantal cases staat op de y-as en de datum als yyyy-mm-dd op de x-as plot_func(data_filtered,&quot;deaths&quot;) (#fig:functie uitvoeren deaths)Figuur 2: Aantal COVID-19 deaths van 01/03/2020 t/m 23/05/2022 in belgië. Aantal cases staat op de y-as en de datum als yyyy-mm-dd op de x-as Uit de gefilterde set en functie zijn twee grafieken gegenereerd. Wat ik interessant vind om te zien bij deze figuren is dat het aantal doden aan het begin het hoogste lag terwijl het aantal cases relatief niet heel hoog waren. Bij de eerste piek van cases is bij het aantal dode ook een piek te zien ronde dezelfde datum. Daarna zwakt het aantal doden vergeleken met de cases af. Of dit komt doordat mensen met een verzwakt imuunsysteem sneller zijn overleden of omdat vaccinaties hun werk gedaan hebben is niet direct op te maken uit deze data, maar het is wel een interessant verband. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
